# -*- coding: utf-8 -*-
"""Dolphin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DgPp_QCEfuuZZpnC6WdeURuG8EPXxcjv
"""

# Commented out IPython magic to ensure Python compatibility.
import networkx as nx
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import random as rand
from networkx.algorithms.community import label_propagation_communities
from networkx.algorithms.community import girvan_newman
from networkx.algorithms.community.modularity_max import greedy_modularity_communities
# %matplotlib inline

J = nx.read_gml("dolphins.gml", destringizer=int)#reading the dolphin social network graphical modelling language file using networkx

#some exploratory analysis here
print(nx.info(J))
print(J.nodes)
print(J.edges)
print(J.adj)

#Visualising the Dolphin social netwok
# Subplot 1
f = plt.figure(figsize=(50,50))
f.tight_layout()
plt.subplot(3, 1, 1)
nx.draw(J,node_size=500,with_labels=True,node_color="skyblue", alpha=0.5, linewidths=10,edge_color="red",font_size=35, font_color="black")
plt.title('Spring Layout (Default)', fontsize=28)

# Subplot 2
f = plt.figure(figsize=(40,40))
f.tight_layout()
plt.subplot(3, 1, 2)
nx.draw_random(J,node_size=500,with_labels=True,node_color="skyblue", alpha=0.5, linewidths=10,edge_color="red",font_size=35, font_color="black")
plt.title('Random Layout', fontsize=28)

# Subplot 3
f = plt.figure(figsize=(40,40))
f.tight_layout()
plt.subplot(3, 1, 3)
nx.draw_shell(J,node_size=500,with_labels=True,node_color="skyblue", alpha=0.5, linewidths=10,edge_color="red",font_size=35, font_color="black")
plt.title('Shell Layout', fontsize=28)

#Edge density of the Dolphin social network
density = nx.density(J)
print('The edge density is: ' + str(density))
list(nx.bridges(J))

#the average degree of the nodes in the Dolphin social network
degree = J.degree()
degree_list = []
degree_dict={}
degree_node=[]
for (n,d) in degree:
    degree_list.append(d)
    degree_node.append(n)

av_degree = sum(degree_list) / len(degree_list)
print('The average degree is ' + str(av_degree))
print('The highest degree is ' + str(max(degree_list)))
dict(zip(degree_node, degree_list))

# #we now plot the degree distribution to get a better insight
fig = plt.figure(figsize=(50,20))
plt.bar(degree_node,degree_list)
plt.show()

#Now we can compute the local clustering coefficient
local_clustering_coefficient = nx.algorithms.cluster.clustering(J)

#lets find the average clustering coefficient
av_local_clustering_coefficient = sum(local_clustering_coefficient.values())/len(local_clustering_coefficient)

#similarly to the degree lets plot the local clustering coefficient distribution
plt.hist(local_clustering_coefficient.values(),label='Local Clustering Coefficient Distribution')
plt.axvline(av_local_clustering_coefficient,color='r',linestyle='dashed',label='Average Local Clustering Coefficient')
plt.legend()
plt.ylabel('Number of Nodes')
plt.title('Local Clustering Coefficient')
plt.show()

#perform the community detection using greedy modularity
c = list(greedy_modularity_communities(J))
#Let's find out how many communities we detected
print("There  are %d community(ies) Based on the the greedy modularity algorithm. Which are as follows: "%(len(c)))
for i,a in enumerate(c): # Loop through the list of fronzenset communities
    if len(a) > 1: # Filter out modularity classes with 2 or fewer nodes
        print('Class '+str(i)+':', list(a)) # Print out the classes and their members

# visualizing the communities
color_map = []
for node in J:
    if node in c[0]:
        color_map.append('blue')
    elif node in c[1]:
        color_map.append('green')
    elif node in c[2]:
        color_map.append('red')
    # elif node in c[4]:
    #     color_map.append('yellow')
    # elif node in c[5]:
    #     color_map.append('black')
    else:
        color_map.append('cyan')
f = plt.figure(figsize=(50,50))
f.tight_layout()
nx.draw(J,node_size=500,with_labels=True,node_color=color_map, alpha=0.5, linewidths=10,edge_color="red",font_size=35, font_color="black")
plt.title('Greedy modularity community detection)', fontsize=28)

#performing community detection with label propagation algorithm
c = list(label_propagation_communities(J))

#Let's find out how many communities we detected
print("There  are %d community(ies) Based on the the greedy modularity algorithm. Which are as follows: "%(len(c)))
for i,a in enumerate(c): # Loop through the list of communities
    if len(a) > 1: # Filter out modularity classes with 2 or fewer nodes
        print('Class '+str(i)+':', list(a)) # Print out the classes and their members

# visualising the communities
color_map = []
for node in J:
    if node in c[0]:
        color_map.append('blue')
    elif node in c[1]:
        color_map.append('green')
    elif node in c[2]:
        color_map.append('red')
    elif node in c[4]:
        color_map.append('yellow')
    elif node in c[5]:
        color_map.append('black')
    else:
        color_map.append('cyan')

f = plt.figure(figsize=(50,50))
f.tight_layout()
nx.draw(J,node_size=500,with_labels=True,node_color=color_map, alpha=0.5, linewidths=10,edge_color="red",font_size=35, font_color="black")
plt.title('Label propagation algorithm', fontsize=28)

list(nx.enumerate_all_cliques(J))

# If your Graph has more than one component, this will return False:
print(nx.is_connected(J))

# Next, use nx.connected_components to get the list of components,
# then use the max() command to find the largest one:
components = nx.connected_components(J)
largest_component = max(components, key=len)

# Create a "subgraph" of just the largest component
# Then calculate the diameter of the subgraph, just like you did with density.
#

subgraph = J.subgraph(largest_component)
diameter = nx.diameter(subgraph)
print("Network diameter of largest component:", diameter)

triadic_closure = nx.transitivity(J)
print("Triadic closure:", triadic_closure)

betweenness_dict = nx.betweenness_centrality(J) # Run betweenness centrality
eigenvector_dict = nx.eigenvector_centrality(J) # Run eigenvector centrality

# Assign each to an attribute in your network
nx.set_node_attributes(J, betweenness_dict, 'betweenness')
nx.set_node_attributes(J, eigenvector_dict, 'eigenvector')

A = nx.to_numpy_matrix(J)
e = nx.eigenvector_centrality(J)
d = nx.degree_centrality(J)
c = nx.closeness_centrality(J)
b = nx.betweenness_centrality(J)

#eigen vector
s_h = {k: v for k, v in sorted(e.items(), key=lambda item: item[1], reverse=True)}
i_aux = 0
new_SH = {}
for k,v in s_h.items():
    if i_aux < 10:
     new_SH[k] = v
     i_aux += 1
    else:
        break
aux1=[]
for k in new_SH.keys():
    aux1.append(k)
d1 = {'Eigenvector' : aux1}

s_h = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}
i_aux = 0
new_SH = {}
for k,v in s_h.items():
    if i_aux < 10:
     new_SH[k] = v
     i_aux += 1
    else:
        break

#degree
aux1=[]
for k in new_SH.keys():
    aux1.append(k)
index = [1,2,3,4,5,6,7,8,9,10]
d2 = {'Degree' : aux1}

s_h = {k: v for k, v in sorted(c.items(), key=lambda item: item[1], reverse=True)}
i_aux = 0
new_SH = {}
for k,v in s_h.items():
    if i_aux < 10:
     new_SH[k] = v
     i_aux += 1
    else:
        break

#closeness
aux1=[]
for k in new_SH.keys():
    aux1.append(k)
d3 = {'Closeness' : aux1}

s_h = {k: v for k, v in sorted(b.items(), key=lambda item: item[1], reverse=True)}
i_aux = 0
new_SH = {}
for k,v in s_h.items():
    if i_aux < 10:
     new_SH[k] = v
     i_aux += 1
    else:
        break

#betweeness
aux1=[]
for k in new_SH.keys():
    aux1.append(k)
d4 = {'Betweenness' : aux1}

def transition(i,j,G,A):
    LDN=list(G.nodes())
    return A[LDN.index(i),LDN.index(j)]/G.degree(i)

def step(i,G,A):
    N=[vecino for vecino in nx.neighbors(G,i)]
    coeficiente_part=[]
    for v in N:
        coeficiente_part.append(transition(i,v,G,A))
    part=[]
    aux=0
    for coef in coeficiente_part:
        part.append(aux+coef)
        aux=aux+coef
    r=np.random.random()
    for p in range(len(part)):
        if p==0:
            if r<part[p]:
                indexf=0
                break
        if p>0:
            if p==len(part)-1:
                indexf=p
                break
            elif (part[p-1]<r  and r<part[p])==True:
                indexf=p
                break
    return N[indexf]

list_nodes = list(J.nodes())

ic=rand.choice(list_nodes) #initial condition
Orbit=[ic]
for i in range(10000):
    ic=step(ic,J,A)
    Orbit.append(ic)

import pylab as plt
node_color=[float(Orbit.count(i)) for i in J]
cmap = plt.cm.coolwarm
plt.figure(figsize=(16,8), dpi=300)
pos = nx.kamada_kawai_layout(J)
#pos = nx.shell_layout(J)
#pos = nx.spectral_layout(J)

nx.draw(J, pos, node_color=node_color, node_size=500, edge_color='gray',
        width=2, with_labels=False, cmap=cmap,alpha=0.9)
for p in pos:
    pos[p][1] -= 0.07
nx.draw_networkx_labels(J, pos)
sm = plt.cm.ScalarMappable(cmap=cmap,norm=plt.Normalize())
sm._A = []
plt.colorbar(sm)
plt.show()

hist_order = dict(zip(list(J.nodes()),node_color))

sorted_hist = {k: v for k, v in sorted(hist_order.items(), key=lambda item: item[1], reverse=True)}
i_aux = 0
new_SH = {}
for k,v in sorted_hist.items():
    if i_aux < 10:
     new_SH[k] = v
     i_aux += 1
    else:
        break

aux1=[]
for k in new_SH.keys():
    aux1.append(k)
index = [1,2,3,4,5,6,7,8,9,10]
d5 = {'Newman' : aux1}
data_Dic = dict(list(d1.items())+list(d2.items())+list(d3.items())+list(d4.items())+list(d5.items()))

dframe = pd.DataFrame(data_Dic,index=index)
dframe.head(10)

H = []
for w in sorted(new_SH, key=new_SH.get, reverse=True):
  H.append(new_SH[w])
H = np.array(H)
H = H/max(H)

pos = np.arange(len(new_SH))
plt.bar(range(len(H)),H, align='center',color= 'r', width=0.6)
plt.xticks(pos, new_SH.keys(),rotation = 45)
plt.ylabel('Centrality')
plt.xlabel('Nodes')
plt.legend(['Newman'])
plt.show()